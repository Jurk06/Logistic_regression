{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_regression_interview_Kit.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkFhpNjzkZEh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu2UZANBkfG6"
      },
      "source": [
        "# What is Logistic Regression?\n",
        "* , logistic regression is the baseline supervised machine learning algorithm for classification,\n",
        "* Statistical Model uses logistic function to model binary dependent variable\n",
        "* Here the outcome is binary variable such as Pass\\Fail, True\\False set\n",
        "* Logistic Regression is method of classification \n",
        "* LR uses Logustic function , also caleed as Sigmoid function \n",
        "     * >f(x)=1 / (1 + e^-x)\n",
        "     * Ranges varies from 0 to 1\n",
        "* Logistic regression models the probability of the default class (e.g. the first class).\n",
        "* Logistic regression is a linear method, but the predictions are transformed using the logistic function.\n",
        "* The probability prediction must be transformed into a binary values (0 or 1) in order to actually make a probability prediction\n",
        "* For more \n",
        "   * >https://web.stanford.edu/~jurafsky/slp3/5.pdf\n",
        "\n",
        "* Logistic regression has two phases:\n",
        "   * >training: we train the system (specifically the weights w and b) using stochastic gradient descent and the cross-entropy loss.\n",
        "    * >test: Given a test example x we compute p(y|x) and return the higher probability\n",
        "    * >label y = 1 or y = 0 \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApwcpAASoCyi"
      },
      "source": [
        "# Sigmoid Function\n",
        "* A sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve. \n",
        "* A common example of a sigmoid function is the logistic function shown in the first figure and defined by the formula\n",
        "   * >S(x)=1/(1+e^-x)\n",
        "   * Value ranges from 0 to 1\n",
        "* It is differentiable at each point which is good for learning rate (step-size)\n",
        "* The sigmoid function has the property\n",
        "1−σ(x) = σ(−x)\n",
        "* Now we have an algorithm that given an instance x computes the probability\n",
        "P(y = 1|x). How do we make a decision? For a test instance x, we say yes if the\n",
        "probability P(y = 1|x) is more than .5, and no otherwise \n",
        "* Decision Boundary "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVVoA6aLouuy"
      },
      "source": [
        "# Squashing (Meaning squeeze)\n",
        "* We will be using sigmoid function to squash the value between 0 and 1.\n",
        "* It give more weight to smaller value and less to large values\n",
        "* Its work is to make the curve smoother\n",
        "* it tends to squash outlier values toward 0 or 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLxGk9sIzCBH"
      },
      "source": [
        "# Importance of Weight Function in Linear Regression\n",
        "* we want to minimize miss-classification and maximize the correct classification.Therefore our task is to find hyper-plane such that yi .WT*Xi >0 for as many points as possible.\n",
        "* The task is to find the optimal values of the weight vector W(i.e. w0, w1, w2, …, wm) using the training data available which is done using the Gradient Descent Algorithm.\n",
        "*  The diance of the point is given by \n",
        "   *  >d=||w^TXi||\n",
        "   * Since Xi is in the direction of W, WT*Xi will be positive (WT*Xi > 0). It implies that Xi will be classified to positive class otherwise is is negative\n",
        "   * Case 1 : If yi =+1\n",
        "\n",
        "      * WT * Xi is +ve\n",
        "      * yi^ =+1 . Implies that point is correctly classified.\n",
        "\n",
        "      * WT * Xi is -ve\n",
        "      * yi^ =-1 . Implies that point is incorrectly classified.\n",
        "    * Case 2: If yi =-1\n",
        "\n",
        "      * WT * Xi is -ve\n",
        "      * yi^ =-1 . Implies that point is incorrectly classified.\n",
        "\n",
        "      * WT * Xi is +ve\n",
        "      * yi^ =+1 . Implies that point is correctly classified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmvZ_Ao-0_uz"
      },
      "source": [
        "# Regularization\n",
        "*  Method to avoid the overfitting \n",
        "* A new term callled Regularization , is used to the loss -function\n",
        "* The new regularization term R(θ) is used to penalize large weights. \n",
        "* To do so there are two type of regularization \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdxGo6EB2X4Z"
      },
      "source": [
        "# L2 Regularizarion \n",
        "* is a quadratic function of the weight values\n",
        "* named because it uses the (square of the) L2 norm of the weight\n",
        "values. \n",
        "*  The L2 norm, ||θ||2, is the same as the Euclidean distance of the vector θ\n",
        "from the origin\n",
        "   * >R(θ) = ||w||^2\n",
        "   * Now the objective function looks like \n",
        "   * w*=argmin sum(i to n) lof(1+exp^{-yw^Txi}) +λ*||w||^2\n",
        "   * λ is hypper parameter\n",
        "   * Hyper parameter tuning(λ = 1/c) and fitting model to logistic regression classifier. By default LR uses the L2 regularization.\n",
        "   *  L2 regularization is called ridge regression,\n",
        "   *  L2 prefers weight vectors with many small weights, \n",
        "   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8Z2t47y4HuX"
      },
      "source": [
        "# L1 Regularization\n",
        "* a linear function of the weight values\n",
        "* named after the L1 norm ||W||1, the sum of the absolute values of the weights, or Manhattan distance\n",
        "* Manhattan distance is the distance you’d have to walk between two points in a city with a street grid like New York)\n",
        "   *R(θ) = ||w||1\n",
        "   * w*=argmin sum(i to n) lof(1+exp^{-yw^Txi}) +λ*||w||1\n",
        "   * It creates sparsity \n",
        "   * The drivative of L1 is constant depens upon the learninf rate (r) , hence \n",
        "   * Optimal value of w* is achieved very eassily\n",
        "   * L1 regularization is called lasso lasso regression\n",
        "   * L1 regularization is more complex (the derivative of |θ| is non-continuous at zero)\n",
        "   * L1 prefers sparse\n",
        "solutions with some larger weights but many more weights set to zero\n",
        "   * Thus L1 regularization leads to much sparser weight vectors, that is, far fewer features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-K2lF6moB-H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}